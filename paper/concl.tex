\section{Future Work}

Future work can be broken down into three broad categories:
\begin{enumerate}
\item \textbf{Expanding}: The functionality of NVKV is extremely limited right
now. There are two major features that are missing that would both be
challenging to implement, but also extremely good candidates for research:
concurrency and larger transactions.

NVKV is currently single-threaded and does not support concurrent operations.
Doing so would be somewhat challenging, since most persistency-safe
transactional memory models do not take concurrent operations into account. The
simple solution is to use locks, resetting them should the power fail. However,
this can limit the multithreaded performance of the application. It would be
interesting to investigate how more fine-grained concurrency would be possible
while remaining consistent across power failures.

Next, NVKV uses transactional memory to ensure its consistency with each
operation, but it does not
provide support for multi-operation transactions like \bdb does. While support
for such transactions could be implemented in a similar manner, it would be
interesting to determine the most viable way to implement multi-operation
transactions.

\item \textbf{Optimizing}: A significant performance loss for insert is in the
data copy-in phase. Each written word of the data is separately persisted before
the next one is written. Language support for a
\textit{persists-with} relationship\footnote{This was my class project in
Programming Languages---defining the math for this to work. It is possible and
can be well defined.} where you could specify that if a particular write $x$
is persisted, then a set of writes $w_0, w_1, ..., w_n$ must also be
persisted, would allow a set of optimizations and improvements that are
extremely easy to use compared to explicit flushing.
This effectively enforces a visible ordering on persistent writes. It can be
implemented with much higher efficiency than explicit cache-line flushing, and
would significantly improve the data copy-in phase.

Another optimization would be for lookup. If each value were tagged with the
``level'' of the hash table that it was locatable by, and this tag were updated
each time it moved, then the fsck tool could also optimize by calculating the
lowest level required for lookup to check before declaring the item not present.
Alternatively, the fsck tool (which is allowed to optimize the database) could
do the reinsert of each item in the table, as long as it first made a backup.

\item \textbf{Exploring}: Finally, the Cuckoo hashing variant presented here has
merit for further exploration and research. An obvious path would be to explore
how other variants of Cuckoo hashing (which, for example, increase the load factor that
the table can tolerate) can interoperate with the no-reinsertion while rehashing
scheme. Finally, a more detailed analysis of this scheme should be done.

\end{enumerate}

\section{Conclusion}

Non-volatile memory opens up the possibility of a new generation of applications
with new, fundamentally different designs compared to applications designed for
a separated storage heirarchy. Key-value stores need to be similarly redesigned
with the new consistency requirements and low-latency access in mind. NVKV
attempts to tackle these challenges through the use of \textit{direct} access to
BNVM and transactional memory to ensure consistency. These two requirements
resulted in a system with small transacation sizes, always-consistent
sub-operations, and bounded steps for insert.

It is possible that transactional memory will appear differently than it is
presented in this paper. However, we expect that the pessimistic nature of our
transactional memory use will allow it to be applicable to a wide variety of
transactional memory implementations. We found that the transactions on memory
required by insert were small, and that other memory operations could be
implemented without transactions. The transaction sizes were both improved and
worsened when compiling with size optimizations, and each transaction was not
optimal in terms of instruction and memory access count, pointing to the need
for language support for transactions. We found that a significant improvement
to the design, code complexity, and performance of the system could be
achieved by using the non-reinsert rehashing scheme described above. Cuckoo
hashing fits well with this scheme, resulting in unexpected positive effects
when using it. Finally, the overall performance of NVKV was significantly
improved over \bdb, likely due to implementing consistency mechanisms taylored
for the technology rather than grafting old mechanisms onto a new storage heirarchy.


Certainly, NVKV has significant design faults and limitations; however, it
clearly demonstrates the need to redesign applications for the underlying
storage technology. This is not a surprise; we have seen this numerous times as
technologies become cheaper or more readily available. However, the longer we
wait, the harder it will be to adopt new designs for applications as the
stop-gap measures that are introduced (such as BNVM presented as block-devices)
become entrenched. We must pave a new way forward with exciting new ideas
to make best use of these technologies and not hold ourselves to designs that
will be relics of the past.







